---
layout: post
title: "Computers and the Augmentation of the Mind"
summary: This post is a brief rundown on the history of artificial intelligence and the drive to augment human thought. I'm writing these as I flesh out ideas related to my doctoral research.
tags: [artificial intelligence, machine intelligence]
---

The concept of thinking machines -- machines capable of something akin to human thought -- has become both a personal and professional interests of mine. The interests stems from Steve Ramsay's course Electronic Text that I took during the Fall 2010 term at the University of Nebraska-Lincoln. We held weekly discussions on readings, and Prof. Ramsay largely allowed the course of our discussions to grow organically: whatever the class latched on to as an important topic for that week would turn up again in the next. We had fascinating discussions about machine intelligence, emotions, logic, the nature of being human. I credit these discussions for planting inside me a fascination with the capabilities of machines to have intelligence.

I began cultivating my scholarly interest in machine intelligence that same semester while also enrolled in a seminar on twentieth century international history. My research paper for the seminar was an analysis of computational theorists developing ideas and concepts for thinking machines during World War II and in the immediate Cold War era (bounded roughly between 1942 to 1960 and spanning the United States, Britain, and the Soviet Union). It is during this period that theories emerged about the seamless integration between humans and machines emerged; cybernetics, information and control systems, and artificial intelligence trace their roots to this period. These early theorists were immediately concerned with the national defense potential that intelligent computers could provide their nations.

The earliest computers placed in the service of defense were targeting mechanisms used on British antiaircraft guns. Even the most skilled of human gunners struggled with determining the appropriate gun lead to have on a speeding, twisting aircraft. The result were thousands of wasted rounds of ammunition and few downed enemy planes. The solution were small electromagnetic analog computers that calculated a plane's probable future position and positioned servomechanisms that allowed the gun to be controlled automatically.

The challenge was automating this process, which required complex trajectory tables and the relations between variables such as caliber, shell size, and fuse, all of which needed computing simultaneously. Norbert Wiener and Oswald Veblen, distinguished mathematics professors at MIT and Princeton respectively, studied these gunnery problems and employed hundreds of people, mostly women, to compute ballistics tables by hand using desk calculators (these staffers were called "computers"). But even with mechanical aid, human "computer" made mistakes, leading to time-consuming and tedious error-checking.

The Ballistics Research Laboratory (BRL) and Moore's School of Engineering in Philadelphia developed ENIAC (Electronic Numerical Interrogator and Computer), America's second full-scale electronic digital computer.[2. The first American digital computer was the Atanasoff-Berry Computer (ABC) developed by John Vincent Atanasoff and Clifford Berry while they were working on World War II assignments. The computer pioneered important computing concepts such as binary arithmetic and electronic switching elements, but was designed for the specific task of calculating linear equations. ENIAC was the world' third digital computer, appearing after both ABC and the British Colossus.] ENIAC was completed in the fall of 1945, missing the chance to apply the machine's capabilities to the war. However, being a military machine, it was immediately applied to the emerging nuclear concerns of the Cold War. The first calculations programmed into the machine were mathematical models for a hydrogen bomb from the Los Alamos atomic bomb laboratories. After several weeks of arranging plug boards and switches (ENIAC was unable to store programs and could not retain more than twenty ten-digit numbers in its memory) and reading data off one million punch cards, the machine exposed several issues with the H-bomb design. Los Alamos laboratories used the data to rebuild their design of the bomb.

While early computer developments were applied to solving weapons of war problems, theoretical computationalists also began testing ideas about the computer's ability to handle a time of crisis. Convinced that humans would be unable to make decisions quickly enough in the event of a nuclear war, computer scientists began seeking theories and methods to make the human brain anew. Theories of machine intelligence gained traction in 1950 under the brilliant work of Alan Turing, an English mathematician, logician, and cryptanalyst. In 1936, Turing published "On Computable Numbers," where he first suggested a machine-like logical procedure for mimicking computations performed by human mathematicians. "The behavior of the [human] computer," he wrote, "at any moment is determined by the symbols which he is observing, and his 'state of mind' at that moment." He compared "a man in the process of computing a real number to a machine which is only capable of a finite number of conditions." Fourteen years later, Turing published "Computing Machinery and Intelligence" in the philosophical journal Mind. Turing asked his readers to envision a game of three people comprised of a man, a woman, and an interrogator of either gender placed in a separate room away from the others. The interrogator was tasked with asking questions of the man and woman and determining the gender of each based on answers alone -- appearance, voice, and other clues for the player would be hidden. Turing then asked to replace one of the genders with a machine and have the interrogator guess which is human and which is not. The paper became the basis of the famous Turing Test to determine whether a computer has intelligence.

Deep Blue and Watson
--------------------

Enter Deep Blue and Watson, two projects of International Business Machines (IBM) to demonstrate the intelligence capabilities of machines.